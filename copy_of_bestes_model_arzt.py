# -*- coding: utf-8 -*-
"""Copy of bestes_model_arzt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G7ImanTG5EAMWMNTpWzfKQS51nCxSWp_
"""

!pip install langchain_openai
!pip install -U langchain-community

!pip install faiss-cpu

# COMPLETE IMPROVED AESTHETIC MEDICINE QA SYSTEM
# ==============================================
# Enhanced version with better chunking, retrieval, and comprehensive answers

import requests
from bs4 import BeautifulSoup
import re
import json
import csv
import os
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ==============================================
# 1. ENHANCED WEB SCRAPER
# ==============================================

class EnhancedScraper:
    def __init__(self, base_url="https://haut-labor.de"):
        self.base_url = base_url

    def fetch_sitemap(self, sitemap_url=None):
        """Fetch URLs from sitemap"""
        if not sitemap_url:
            sitemap_url = f"{self.base_url}/sitemap.xml"

        try:
            resp = requests.get(sitemap_url, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "xml")
            urls = [loc.text for loc in soup.find_all("loc")
                   if loc.text.startswith(self.base_url)]
            print(f"✅ Found {len(urls)} URLs in sitemap")
            return urls
        except Exception as e:
            print(f"❌ Sitemap fetch failed: {e}")
            return []

    def fetch_pages(self, max_pages=None):
        """Fetch page content with enhanced error handling"""
        urls = self.fetch_sitemap()
        if max_pages:
            urls = urls[:max_pages]

        pages = {}
        failed_urls = []

        print(f"🔄 Fetching {len(urls)} pages...")
        for i, url in enumerate(urls, 1):
            try:
                resp = requests.get(url, timeout=15)
                resp.raise_for_status()
                pages[url] = resp.text
                if i % 10 == 0:
                    print(f"   Progress: {i}/{len(urls)} pages")
            except Exception as e:
                failed_urls.append(url)
                print(f"⚠️  Failed {url}: {e}")

        print(f"✅ Successfully fetched {len(pages)} pages")
        if failed_urls:
            print(f"❌ Failed to fetch {len(failed_urls)} pages")

        return pages

# ==============================================
# 2. ENHANCED TEXT CLEANER
# ==============================================

class EnhancedCleaner:
    @staticmethod
    def extract_clean_text(html):
        """Extract and clean text with better preservation of structure"""
        soup = BeautifulSoup(html, "html.parser")

        # Remove unwanted elements
        for tag in soup(["script", "style", "nav", "footer", "aside", "form", "noscript"]):
            tag.decompose()

        # Get text with better formatting
        text = soup.get_text(separator="\n")

        # Clean up whitespace while preserving structure
        text = re.sub(r'\n\s*\n+', '\n\n', text)  # Multiple newlines to double
        text = re.sub(r'[ \t]+', ' ', text)           # Multiple spaces to single
        text = text.strip()

        return text

# ==============================================
# 3. ENHANCED DOCUMENT BUILDER
# ==============================================

class EnhancedDocumentBuilder:
    def __init__(self):
        self.treatment_summary = self._create_treatment_summary()

    def _create_treatment_summary(self):
        """Create comprehensive treatment summary"""
        summary = """Das Hautlabor Dr. med. Lara Pfahl bietet folgende ästhetische Behandlungen an:

GESICHTSBEHANDLUNGEN (17 Behandlungen):
• CO₂-Laser - Hauterneuerung und Faltenreduktion durch fraktionierte Laserbehandlung
• Fadenlifting - Nicht-operative Gesichtsstraffung mit resorbierbaren Fäden
• Faltenrelaxan (Botox) - Muskelentspannung gegen Mimikfalten
• Filler (Hyaluronsäure) - Volumenaufbau und Faltenglättung
• HydraFacial - Tiefenreinigung, Peeling und Hydratation in einem
• LaseMD - Hautverbesserung und Anti-Aging mit fraktioniertem Laser
• Lidstraffung ohne OP - Nicht-operative Straffung der Augenpartie
• Lumecca - IPL-Behandlung gegen Pigmentflecken und Rötungen
• Morpheus8 - Microneedling mit Radiofrequenz für Hautstraffung
• Polynukleotide - Innovative Hautregeneration und Anti-Aging
• Radiesse® - Kollagenstimulation für natürliche Hautstraffung
• Sculptra® - Langanhaltender Volumenaufbau durch Kollagenbildung
• SkinPen - Professionelles Microneedling für Hautverbesserung
• Skinbooster - Hautfeuchtigkeit und -qualität verbessern
• Ultherapy - Ultraschall-Lifting ohne Operation
• Vampirlifting (PRP) - Eigenbluttherapie für Hautverjüngung
• VectraH2 Hautanalyse - Professionelle Hautdiagnostik

KÖRPERBEHANDLUNGEN (6 Behandlungen):
• CO₂-Laser Körper - Hautverbesserung und Narbenbehandlung am Körper
• Haarentfernung - Dauerhafte Haarentfernung mit modernster Lasertechnik
• Lipolyse - Nicht-operative Fettreduktion
• Morpheus8 Körper - Hautstraffung und Cellulite-Behandlung am Körper
• Sculptra® für Hip Dips & Po - Körperkonturierung und Volumenaufbau
• Ästhetische Gynäkologie - Spezialisierte Intimbereich-Behandlungen

BEHANDLUNGEN FÜR MÄNNER (7 Behandlungen):
• Dauerhafte Haarentfernung Männer - Professionelle Laser-Haarentfernung
• Faltenbehandlung Männer - Botox und Filler speziell für Männer
• LaseMD Männer - Hautverbesserung und Anti-Aging für Männer
• Lumecca Männer - IPL-Behandlung gegen Pigmentflecken
• Morpheus8 Männer - Hautstraffung mit Radiofrequenz
• PRP-Haarwachstumstherapie Männer - Eigenbluttherapie gegen Haarausfall
• Radiesse® Männer - Kollagenstimulation für männliche Haut

GESAMT: 30 verschiedene ästhetische Behandlungen
Alle Behandlungen werden von Dr. med. Lara Pfahl und ihrem erfahrenen Team durchgeführt.
Praxis: Hautlabor Oldenburg, spezialisiert auf ästhetische Medizin und Hautbehandlungen."""

        return Document(
            page_content=summary,
            metadata={
                "source": "comprehensive_treatment_list",
                "type": "summary",
                "priority": "highest"
            }
        )

    def build_enhanced_docs(self, pages, min_words=30):
        """Build documents with enhanced metadata"""
        docs = []
        cleaner = EnhancedCleaner()

        for url, html in pages.items():
            clean_text = cleaner.extract_clean_text(html)

            if len(clean_text.split()) >= min_words:
                # Enhanced metadata
                metadata = {
                    "source": url,
                    "word_count": len(clean_text.split()),
                    "char_count": len(clean_text)
                }

                # Classify document type
                if url.endswith("haut-labor.de/"):
                    metadata["type"] = "homepage"
                    metadata["priority"] = "high"
                elif "/behandlung/" in url:
                    metadata["type"] = "treatment_page"
                    metadata["priority"] = "high"
                    # Extract treatment name
                    treatment_match = re.search(r'/behandlung/([^/]+)', url)
                    if treatment_match:
                        metadata["treatment"] = treatment_match.group(1)
                elif "/beratung" in url:
                    metadata["type"] = "consultation"
                    metadata["priority"] = "medium"
                else:
                    metadata["type"] = "general"
                    metadata["priority"] = "low"

                # Mark treatment-related content
                treatment_keywords = [
                    "CO₂-Laser", "Fadenlifting", "Morpheus8", "Ultherapy",
                    "Sculptra", "Radiesse", "Haarentfernung", "Botox", "Filler"
                ]
                metadata["has_treatments"] = any(
                    keyword.lower() in clean_text.lower()
                    for keyword in treatment_keywords
                )

                docs.append(Document(page_content=clean_text, metadata=metadata))

        # Add treatment summary
        docs.append(self.treatment_summary)

        return docs

# ==============================================
# 4. ENHANCED TEXT SPLITTER
# ==============================================

class EnhancedSplitter:
    def __init__(self):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # Larger chunks for better context
            chunk_overlap=200,    # More overlap for continuity
            separators=[
                "\n\n\n",        # Major section breaks
                "\n\n",          # Paragraph breaks
                "\nBehandlungen\n", # Treatment sections
                "\nGesicht\n",    # Category headers
                "\nKörper\n",
                "\nMänner\n",
                "\n• ",           # List items
                "\n",             # Line breaks
                ". ",              # Sentences
                " ",               # Words
                ""                 # Characters
            ],
            keep_separator=True
        )

    def split_documents(self, docs):
        """Split documents with enhanced metadata preservation"""
        chunks = self.splitter.split_documents(docs)

        # Enhance chunk metadata
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i

            # Calculate importance score
            importance = 0
            content_lower = chunk.page_content.lower()

            # High priority for summary and homepage
            if chunk.metadata.get("type") in ["summary", "homepage"]:
                importance += 20
            elif chunk.metadata.get("type") == "treatment_page":
                importance += 15

            # Boost for treatment mentions
            treatment_count = sum(1 for treatment in [
                "co₂-laser", "morpheus8", "ultherapy", "sculptra",
                "radiesse", "botox", "filler", "haarentfernung"
            ] if treatment in content_lower)
            importance += treatment_count * 3

            # Boost for list-like content
            if "•" in chunk.page_content or chunk.page_content.count("\n") > 5:
                importance += 5

            chunk.metadata["importance"] = importance

        return chunks

# ==============================================
# 5. ENHANCED QA SYSTEM
# ==============================================

class EnhancedQASystem:
    def __init__(self, api_key):
        os.environ["OPENAI_API_KEY"] = api_key
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model_name="gpt-4", temperature=0)
        self.faiss_index = None
        self.qa_chain = None

        # Custom prompt for better answers
        self.custom_prompt = PromptTemplate(
            template="""Du bist Dr. med. Lara Pfahl's AI-Assistent für das Hautlabor in Oldenburg. Du hilfst Patienten bei Fragen zu ästhetischen Behandlungen.

KONTEXT:
{context}

FRAGE: {question}

ANWEISUNGEN:
- Gib vollständige, strukturierte Antworten
- Bei Behandlungslisten: Zeige alle verfügbaren Optionen
- Erwähne spezifische Details (Preise, Dauer, Nachsorge) wenn verfügbar
- Sei medizinisch präzise aber verständlich
- Verweise bei Bedarf auf Beratungstermine
- Nutze die Informationen aus mehreren Quellen für umfassende Antworten

ANTWORT:""",
            input_variables=["context", "question"]
        )

    def build_index(self, chunks):
        """Build FAISS index with enhanced retrieval"""
        print(f"🔄 Building FAISS index with {len(chunks)} chunks...")

        self.faiss_index = FAISS.from_documents(chunks, self.embeddings)

        # Save index
        self.faiss_index.save_local("enhanced_faiss_index")
        print("✅ FAISS index built and saved")

    def load_index(self):
        """Load existing FAISS index"""
        try:
            self.faiss_index = FAISS.load_local(
                "enhanced_faiss_index",
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("✅ FAISS index loaded")
        except Exception as e:
            print(f"❌ Failed to load index: {e}")

    def create_qa_chain(self):
        """Create enhanced QA chain"""
        if not self.faiss_index:
            raise ValueError("FAISS index not built or loaded")

        # Enhanced retriever with MMR
        retriever = self.faiss_index.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance
            search_kwargs={
                "k": 15,           # Retrieve more chunks
                "fetch_k": 30,     # Consider more candidates
                "lambda_mult": 0.7 # Balance relevance vs diversity
            }
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": self.custom_prompt}
        )

        print("✅ Enhanced QA chain created")

    def query(self, question):
        """Query the system with enhanced output"""
        if not self.qa_chain:
            raise ValueError("QA chain not created")

        print(f"\n{'='*60}")
        print(f"FRAGE: {question}")
        print('='*60)

        result = self.qa_chain.invoke({"query": question})

        print("ANTWORT:")
        print("-" * 40)
        print(result["result"])

        print(f"\n{'='*60}")
        print("QUELLEN:")
        print('='*60)

        # Show top sources with metadata
        for i, doc in enumerate(result["source_documents"][:5], 1):
            source = doc.metadata.get("source", "Unbekannt")
            doc_type = doc.metadata.get("type", "general")
            importance = doc.metadata.get("importance", 0)

            print(f"{i}. {source}")
            print(f"   Typ: {doc_type} | Wichtigkeit: {importance}")
            print(f"   Text: {doc.page_content[:100]}...")
            print()

        return result

# ==============================================
# 6. COMPLETE PIPELINE
# ==============================================

def run_complete_pipeline(api_key, max_pages=200):
    """Run the complete enhanced pipeline"""

    print("🚀 STARTING ENHANCED AESTHETIC MEDICINE QA SYSTEM")
    print("=" * 60)

    # Step 1: Scrape website
    print("\n1️⃣  SCRAPING WEBSITE")
    scraper = EnhancedScraper()
    pages = scraper.fetch_pages(max_pages=max_pages)

    # Step 2: Build documents
    print("\n2️⃣  BUILDING DOCUMENTS")
    builder = EnhancedDocumentBuilder()
    docs = builder.build_enhanced_docs(pages)
    print(f"✅ Built {len(docs)} documents")

    # Step 3: Split into chunks
    print("\n3️⃣  CREATING ENHANCED CHUNKS")
    splitter = EnhancedSplitter()
    chunks = splitter.split_documents(docs)
    print(f"✅ Created {len(chunks)} enhanced chunks")

    # Step 4: Save data
    print("\n4️⃣  SAVING DATA")
    save_enhanced_data(chunks)

    # Step 5: Build QA system
    print("\n5️⃣  BUILDING QA SYSTEM")
    qa_system = EnhancedQASystem(api_key)
    qa_system.build_index(chunks)
    qa_system.create_qa_chain()

    # Step 6: Test system
    print("\n6️⃣  TESTING SYSTEM")
    test_questions = [
        "Welche Behandlungen werden von Ihnen angeboten?",
        "Was kostet eine Morpheus8 Behandlung?",
        "Welche Behandlungen gibt es speziell für Männer?",
        "Wie funktioniert Ultherapy und wie lange dauert es?",
        "Welche Behandlungen helfen gegen Falten?"
    ]

    for question in test_questions:
        qa_system.query(question)
        input("\nPress Enter for next question...")

    print("\n🎉 SYSTEM READY!")
    return qa_system

def save_enhanced_data(chunks):
    """Save enhanced data to files"""

    # Save JSON
    json_data = []
    for chunk in chunks:
        json_data.append({
            "chunk_id": chunk.metadata.get("chunk_id", 0),
            "text": chunk.page_content,
            "metadata": chunk.metadata
        })

    with open("enhanced_kb_chunks.json", "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    # Save CSV
    with open("enhanced_kb_chunks.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "chunk_id", "text", "source", "type", "importance", "has_treatments"
        ])
        writer.writeheader()

        for chunk in chunks:
            writer.writerow({
                "chunk_id": chunk.metadata.get("chunk_id", 0),
                "text": chunk.page_content,
                "source": chunk.metadata.get("source", ""),
                "type": chunk.metadata.get("type", ""),
                "importance": chunk.metadata.get("importance", 0),
                "has_treatments": chunk.metadata.get("has_treatments", False)
            })

    print("✅ Data saved to enhanced_kb_chunks.json and enhanced_kb_chunks.csv")

# ==============================================
# 7. USAGE EXAMPLE
# ==============================================

if __name__ == "__main__":
    # Set your OpenAI API key here
    API_KEY = os.getenv('OPENAI_API_KEY')
    if not API_KEY:
        raise ValueError("OPENAI_API_KEY environment variable is required")

    # Run the complete pipeline
    qa_system = run_complete_pipeline(API_KEY, max_pages=50)  # Start with 50 pages for testing

    # Interactive mode
    print("\n" + "="*60)
    print("INTERACTIVE MODE - Ask your questions!")
    print("Type 'quit' to exit")
    print("="*60)

    while True:
        question = input("\nYour question: ")
        if question.lower() in ['quit', 'exit', 'q']:
            break

        try:
            qa_system.query(question)
        except Exception as e:
            print(f"Error: {e}")

    print("\n👋 Goodbye!")

# Create an improved chatbot implementation

import os
import json
import pandas as pd
from datetime import datetime
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.schema import Document

class ImprovedChatbot:
    def __init__(self, api_key, csv_file="enhanced_kb_chunks.csv"):
        """Initialize improved chatbot with memory and faster responses"""
        os.environ["OPENAI_API_KEY"] = api_key

        # Use faster, cheaper model
        self.llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",  # Much faster than gpt-4
            temperature=0.1,
            max_tokens=500  # Limit response length for speed
        )

        self.embeddings = OpenAIEmbeddings()

        # Add conversation memory (fixes memory issue)
        self.memory = ConversationBufferWindowMemory(
            k=5,  # Remember last 5 exchanges
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

        # Load and filter data
        self.load_and_filter_data(csv_file)
        self.build_vectorstore()
        self.create_chain()

    def load_and_filter_data(self, csv_file):
        """Load and filter chunks to improve quality"""
        df = pd.read_csv(csv_file)

        # Filter out low-quality chunks
        filtered_df = df[
            (df['importance'] >= 15) |  # High importance chunks
            (df['has_treatments'] == True) |  # Treatment-related content
            (df['type'].isin(['treatment_page', 'homepage', 'summary']))  # Important page types
        ].copy()

        # Remove navigation/menu noise
        filtered_df = filtered_df[
            ~filtered_df['text'].str.contains('Schließen|Startseite|Über uns', na=False, case=False)
        ]

        print(f"Filtered from {len(df)} to {len(filtered_df)} high-quality chunks")

        # Convert to documents
        self.documents = []
        for _, row in filtered_df.iterrows():
            doc = Document(
                page_content=row['text'],
                metadata={
                    'source': row['source'],
                    'type': row['type'],
                    'importance': row['importance']
                }
            )
            self.documents.append(doc)

    def build_vectorstore(self):
        """Build optimized vector store"""
        self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)
        print(f"Built vectorstore with {len(self.documents)} documents")

    def create_chain(self):
        """Create conversational chain with memory"""

        # Improved prompt template
        custom_prompt = PromptTemplate(
            template="""Du bist der AI-Assistent von Dr. med. Lara Pfahl für das Hautlabor Oldenburg.

Verwende den folgenden Kontext und die Gesprächshistorie, um die Frage zu beantworten:

Kontext: {context}

Gesprächshistorie: {chat_history}

Aktuelle Frage: {question}

Anweisungen:
- Gib präzise, hilfreiche Antworten auf Deutsch
- Bei Behandlungsfragen: nenne Kosten, Dauer und Ablauf wenn verfügbar
- Verweise auf Beratungstermine: +49 (0) 157 834 488 90
- Sei freundlich und professionell
- Halte Antworten fokussiert (max. 3-4 Sätze)

Antwort:""",
            input_variables=["context", "chat_history", "question"]
        )

        # Optimized retriever (faster)
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",  # Faster than MMR
            search_kwargs={"k": 5}  # Fewer chunks = faster
        )

        # Create conversational chain
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": custom_prompt},
            return_source_documents=True,
            verbose=False
        )

        print("✅ Conversational chain created with memory")

    def chat(self, question):
        """Chat with memory and context"""
        try:
            # Get response
            result = self.chain.invoke({"question": question})

            # Format response
            answer = result["answer"]
            sources = result.get("source_documents", [])

            print(f"\\n🤖 Antwort: {answer}")

            if sources:
                print(f"\\n📚 Quellen:")
                for i, doc in enumerate(sources[:2], 1):  # Show top 2 sources
                    source = doc.metadata.get('source', 'Unbekannt')
                    print(f"  {i}. {source}")

            return answer

        except Exception as e:
            error_msg = f"Entschuldigung, es gab einen Fehler: {str(e)}"
            print(f"❌ {error_msg}")
            return error_msg

    def reset_memory(self):
        """Reset conversation memory"""
        self.memory.clear()
        print("🧠 Gesprächshistorie zurückgesetzt")

# Usage example
def run_improved_chatbot():
    """Run the improved chatbot"""

    # Initialize chatbot
    API_KEY = os.getenv('OPENAI_API_KEY')
    if not API_KEY:
        raise ValueError("OPENAI_API_KEY environment variable is required")

    try:
        chatbot = ImprovedChatbot(API_KEY)

        print("\\n" + "="*60)
        print("🚀 VERBESSERTER HAUTLABOR CHATBOT")
        print("="*60)
        print("Verbesserungen:")
        print("✅ Schnellere Antworten (GPT-3.5-turbo)")
        print("✅ Gesprächsgedächtnis (5 letzte Nachrichten)")
        print("✅ Bessere Datenqualität (gefilterte Chunks)")
        print("✅ Fokussierte Antworten")
        print("\\nBefehle: 'reset' (Gedächtnis löschen), 'quit' (beenden)")
        print("="*60)

        # Interactive chat loop
        while True:
            question = input("\\n👤 Ihre Frage: ").strip()

            if question.lower() in ['quit', 'exit', 'q']:
                print("\\n👋 Auf Wiedersehen!")
                break
            elif question.lower() == 'reset':
                chatbot.reset_memory()
                continue
            elif not question:
                continue

            # Get response
            chatbot.chat(question)

    except Exception as e:
        print(f"❌ Fehler beim Starten: {e}")
        print("Bitte überprüfen Sie Ihren OpenAI API Key")

if __name__ == "__main__":
    run_improved_chatbot()


# Save the improved code to a file
with open('improved_chatbot.py', 'w', encoding='utf-8') as f:
    f.write(improved_code)

print("✅ Improved chatbot code saved to 'improved_chatbot.py'")
print("\nKey improvements:")
print("1. 🚀 Speed: GPT-3.5-turbo instead of GPT-4")
print("2. 🧠 Memory: ConversationBufferWindowMemory")
print("3. 🎯 Quality: Filtered chunks, better prompts")
print("4. ⚡ Efficiency: Fewer retrieved chunks (5 vs 15)")