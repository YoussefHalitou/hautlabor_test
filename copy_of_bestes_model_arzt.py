# -*- coding: utf-8 -*-
"""Copy of bestes_model_arzt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G7ImanTG5EAMWMNTpWzfKQS51nCxSWp_
"""

!pip install langchain_openai
!pip install -U langchain-community

!pip install faiss-cpu

# COMPLETE IMPROVED AESTHETIC MEDICINE QA SYSTEM
# ==============================================
# Enhanced version with better chunking, retrieval, and comprehensive answers

import requests
from bs4 import BeautifulSoup
import re
import json
import csv
import os
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ==============================================
# 1. ENHANCED WEB SCRAPER
# ==============================================

class EnhancedScraper:
    def __init__(self, base_url="https://haut-labor.de"):
        self.base_url = base_url

    def fetch_sitemap(self, sitemap_url=None):
        """Fetch URLs from sitemap"""
        if not sitemap_url:
            sitemap_url = f"{self.base_url}/sitemap.xml"

        try:
            resp = requests.get(sitemap_url, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "xml")
            urls = [loc.text for loc in soup.find_all("loc")
                   if loc.text.startswith(self.base_url)]
            print(f"‚úÖ Found {len(urls)} URLs in sitemap")
            return urls
        except Exception as e:
            print(f"‚ùå Sitemap fetch failed: {e}")
            return []

    def fetch_pages(self, max_pages=None):
        """Fetch page content with enhanced error handling"""
        urls = self.fetch_sitemap()
        if max_pages:
            urls = urls[:max_pages]

        pages = {}
        failed_urls = []

        print(f"üîÑ Fetching {len(urls)} pages...")
        for i, url in enumerate(urls, 1):
            try:
                resp = requests.get(url, timeout=15)
                resp.raise_for_status()
                pages[url] = resp.text
                if i % 10 == 0:
                    print(f"   Progress: {i}/{len(urls)} pages")
            except Exception as e:
                failed_urls.append(url)
                print(f"‚ö†Ô∏è  Failed {url}: {e}")

        print(f"‚úÖ Successfully fetched {len(pages)} pages")
        if failed_urls:
            print(f"‚ùå Failed to fetch {len(failed_urls)} pages")

        return pages

# ==============================================
# 2. ENHANCED TEXT CLEANER
# ==============================================

class EnhancedCleaner:
    @staticmethod
    def extract_clean_text(html):
        """Extract and clean text with better preservation of structure"""
        soup = BeautifulSoup(html, "html.parser")

        # Remove unwanted elements
        for tag in soup(["script", "style", "nav", "footer", "aside", "form", "noscript"]):
            tag.decompose()

        # Get text with better formatting
        text = soup.get_text(separator="\n")

        # Clean up whitespace while preserving structure
        text = re.sub(r'\n\s*\n+', '\n\n', text)  # Multiple newlines to double
        text = re.sub(r'[ \t]+', ' ', text)           # Multiple spaces to single
        text = text.strip()

        return text

# ==============================================
# 3. ENHANCED DOCUMENT BUILDER
# ==============================================

class EnhancedDocumentBuilder:
    def __init__(self):
        self.treatment_summary = self._create_treatment_summary()

    def _create_treatment_summary(self):
        """Create comprehensive treatment summary"""
        summary = """Das Hautlabor Dr. med. Lara Pfahl bietet folgende √§sthetische Behandlungen an:

GESICHTSBEHANDLUNGEN (17 Behandlungen):
‚Ä¢ CO‚ÇÇ-Laser - Hauterneuerung und Faltenreduktion durch fraktionierte Laserbehandlung
‚Ä¢ Fadenlifting - Nicht-operative Gesichtsstraffung mit resorbierbaren F√§den
‚Ä¢ Faltenrelaxan (Botox) - Muskelentspannung gegen Mimikfalten
‚Ä¢ Filler (Hyalurons√§ure) - Volumenaufbau und Faltengl√§ttung
‚Ä¢ HydraFacial - Tiefenreinigung, Peeling und Hydratation in einem
‚Ä¢ LaseMD - Hautverbesserung und Anti-Aging mit fraktioniertem Laser
‚Ä¢ Lidstraffung ohne OP - Nicht-operative Straffung der Augenpartie
‚Ä¢ Lumecca - IPL-Behandlung gegen Pigmentflecken und R√∂tungen
‚Ä¢ Morpheus8 - Microneedling mit Radiofrequenz f√ºr Hautstraffung
‚Ä¢ Polynukleotide - Innovative Hautregeneration und Anti-Aging
‚Ä¢ Radiesse¬Æ - Kollagenstimulation f√ºr nat√ºrliche Hautstraffung
‚Ä¢ Sculptra¬Æ - Langanhaltender Volumenaufbau durch Kollagenbildung
‚Ä¢ SkinPen - Professionelles Microneedling f√ºr Hautverbesserung
‚Ä¢ Skinbooster - Hautfeuchtigkeit und -qualit√§t verbessern
‚Ä¢ Ultherapy - Ultraschall-Lifting ohne Operation
‚Ä¢ Vampirlifting (PRP) - Eigenbluttherapie f√ºr Hautverj√ºngung
‚Ä¢ VectraH2 Hautanalyse - Professionelle Hautdiagnostik

K√ñRPERBEHANDLUNGEN (6 Behandlungen):
‚Ä¢ CO‚ÇÇ-Laser K√∂rper - Hautverbesserung und Narbenbehandlung am K√∂rper
‚Ä¢ Haarentfernung - Dauerhafte Haarentfernung mit modernster Lasertechnik
‚Ä¢ Lipolyse - Nicht-operative Fettreduktion
‚Ä¢ Morpheus8 K√∂rper - Hautstraffung und Cellulite-Behandlung am K√∂rper
‚Ä¢ Sculptra¬Æ f√ºr Hip Dips & Po - K√∂rperkonturierung und Volumenaufbau
‚Ä¢ √Ñsthetische Gyn√§kologie - Spezialisierte Intimbereich-Behandlungen

BEHANDLUNGEN F√úR M√ÑNNER (7 Behandlungen):
‚Ä¢ Dauerhafte Haarentfernung M√§nner - Professionelle Laser-Haarentfernung
‚Ä¢ Faltenbehandlung M√§nner - Botox und Filler speziell f√ºr M√§nner
‚Ä¢ LaseMD M√§nner - Hautverbesserung und Anti-Aging f√ºr M√§nner
‚Ä¢ Lumecca M√§nner - IPL-Behandlung gegen Pigmentflecken
‚Ä¢ Morpheus8 M√§nner - Hautstraffung mit Radiofrequenz
‚Ä¢ PRP-Haarwachstumstherapie M√§nner - Eigenbluttherapie gegen Haarausfall
‚Ä¢ Radiesse¬Æ M√§nner - Kollagenstimulation f√ºr m√§nnliche Haut

GESAMT: 30 verschiedene √§sthetische Behandlungen
Alle Behandlungen werden von Dr. med. Lara Pfahl und ihrem erfahrenen Team durchgef√ºhrt.
Praxis: Hautlabor Oldenburg, spezialisiert auf √§sthetische Medizin und Hautbehandlungen."""

        return Document(
            page_content=summary,
            metadata={
                "source": "comprehensive_treatment_list",
                "type": "summary",
                "priority": "highest"
            }
        )

    def build_enhanced_docs(self, pages, min_words=30):
        """Build documents with enhanced metadata"""
        docs = []
        cleaner = EnhancedCleaner()

        for url, html in pages.items():
            clean_text = cleaner.extract_clean_text(html)

            if len(clean_text.split()) >= min_words:
                # Enhanced metadata
                metadata = {
                    "source": url,
                    "word_count": len(clean_text.split()),
                    "char_count": len(clean_text)
                }

                # Classify document type
                if url.endswith("haut-labor.de/"):
                    metadata["type"] = "homepage"
                    metadata["priority"] = "high"
                elif "/behandlung/" in url:
                    metadata["type"] = "treatment_page"
                    metadata["priority"] = "high"
                    # Extract treatment name
                    treatment_match = re.search(r'/behandlung/([^/]+)', url)
                    if treatment_match:
                        metadata["treatment"] = treatment_match.group(1)
                elif "/beratung" in url:
                    metadata["type"] = "consultation"
                    metadata["priority"] = "medium"
                else:
                    metadata["type"] = "general"
                    metadata["priority"] = "low"

                # Mark treatment-related content
                treatment_keywords = [
                    "CO‚ÇÇ-Laser", "Fadenlifting", "Morpheus8", "Ultherapy",
                    "Sculptra", "Radiesse", "Haarentfernung", "Botox", "Filler"
                ]
                metadata["has_treatments"] = any(
                    keyword.lower() in clean_text.lower()
                    for keyword in treatment_keywords
                )

                docs.append(Document(page_content=clean_text, metadata=metadata))

        # Add treatment summary
        docs.append(self.treatment_summary)

        return docs

# ==============================================
# 4. ENHANCED TEXT SPLITTER
# ==============================================

class EnhancedSplitter:
    def __init__(self):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # Larger chunks for better context
            chunk_overlap=200,    # More overlap for continuity
            separators=[
                "\n\n\n",        # Major section breaks
                "\n\n",          # Paragraph breaks
                "\nBehandlungen\n", # Treatment sections
                "\nGesicht\n",    # Category headers
                "\nK√∂rper\n",
                "\nM√§nner\n",
                "\n‚Ä¢ ",           # List items
                "\n",             # Line breaks
                ". ",              # Sentences
                " ",               # Words
                ""                 # Characters
            ],
            keep_separator=True
        )

    def split_documents(self, docs):
        """Split documents with enhanced metadata preservation"""
        chunks = self.splitter.split_documents(docs)

        # Enhance chunk metadata
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i

            # Calculate importance score
            importance = 0
            content_lower = chunk.page_content.lower()

            # High priority for summary and homepage
            if chunk.metadata.get("type") in ["summary", "homepage"]:
                importance += 20
            elif chunk.metadata.get("type") == "treatment_page":
                importance += 15

            # Boost for treatment mentions
            treatment_count = sum(1 for treatment in [
                "co‚ÇÇ-laser", "morpheus8", "ultherapy", "sculptra",
                "radiesse", "botox", "filler", "haarentfernung"
            ] if treatment in content_lower)
            importance += treatment_count * 3

            # Boost for list-like content
            if "‚Ä¢" in chunk.page_content or chunk.page_content.count("\n") > 5:
                importance += 5

            chunk.metadata["importance"] = importance

        return chunks

# ==============================================
# 5. ENHANCED QA SYSTEM
# ==============================================

class EnhancedQASystem:
    def __init__(self, api_key):
        os.environ["OPENAI_API_KEY"] = api_key
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model_name="gpt-4", temperature=0)
        self.faiss_index = None
        self.qa_chain = None

        # Custom prompt for better answers
        self.custom_prompt = PromptTemplate(
            template="""Du bist Dr. med. Lara Pfahl's AI-Assistent f√ºr das Hautlabor in Oldenburg. Du hilfst Patienten bei Fragen zu √§sthetischen Behandlungen.

KONTEXT:
{context}

FRAGE: {question}

ANWEISUNGEN:
- Gib vollst√§ndige, strukturierte Antworten
- Bei Behandlungslisten: Zeige alle verf√ºgbaren Optionen
- Erw√§hne spezifische Details (Preise, Dauer, Nachsorge) wenn verf√ºgbar
- Sei medizinisch pr√§zise aber verst√§ndlich
- Verweise bei Bedarf auf Beratungstermine
- Nutze die Informationen aus mehreren Quellen f√ºr umfassende Antworten

ANTWORT:""",
            input_variables=["context", "question"]
        )

    def build_index(self, chunks):
        """Build FAISS index with enhanced retrieval"""
        print(f"üîÑ Building FAISS index with {len(chunks)} chunks...")

        self.faiss_index = FAISS.from_documents(chunks, self.embeddings)

        # Save index
        self.faiss_index.save_local("enhanced_faiss_index")
        print("‚úÖ FAISS index built and saved")

    def load_index(self):
        """Load existing FAISS index"""
        try:
            self.faiss_index = FAISS.load_local(
                "enhanced_faiss_index",
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("‚úÖ FAISS index loaded")
        except Exception as e:
            print(f"‚ùå Failed to load index: {e}")

    def create_qa_chain(self):
        """Create enhanced QA chain"""
        if not self.faiss_index:
            raise ValueError("FAISS index not built or loaded")

        # Enhanced retriever with MMR
        retriever = self.faiss_index.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance
            search_kwargs={
                "k": 15,           # Retrieve more chunks
                "fetch_k": 30,     # Consider more candidates
                "lambda_mult": 0.7 # Balance relevance vs diversity
            }
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": self.custom_prompt}
        )

        print("‚úÖ Enhanced QA chain created")

    def query(self, question):
        """Query the system with enhanced output"""
        if not self.qa_chain:
            raise ValueError("QA chain not created")

        print(f"\n{'='*60}")
        print(f"FRAGE: {question}")
        print('='*60)

        result = self.qa_chain.invoke({"query": question})

        print("ANTWORT:")
        print("-" * 40)
        print(result["result"])

        print(f"\n{'='*60}")
        print("QUELLEN:")
        print('='*60)

        # Show top sources with metadata
        for i, doc in enumerate(result["source_documents"][:5], 1):
            source = doc.metadata.get("source", "Unbekannt")
            doc_type = doc.metadata.get("type", "general")
            importance = doc.metadata.get("importance", 0)

            print(f"{i}. {source}")
            print(f"   Typ: {doc_type} | Wichtigkeit: {importance}")
            print(f"   Text: {doc.page_content[:100]}...")
            print()

        return result

# ==============================================
# 6. COMPLETE PIPELINE
# ==============================================

def run_complete_pipeline(api_key, max_pages=200):
    """Run the complete enhanced pipeline"""

    print("üöÄ STARTING ENHANCED AESTHETIC MEDICINE QA SYSTEM")
    print("=" * 60)

    # Step 1: Scrape website
    print("\n1Ô∏è‚É£  SCRAPING WEBSITE")
    scraper = EnhancedScraper()
    pages = scraper.fetch_pages(max_pages=max_pages)

    # Step 2: Build documents
    print("\n2Ô∏è‚É£  BUILDING DOCUMENTS")
    builder = EnhancedDocumentBuilder()
    docs = builder.build_enhanced_docs(pages)
    print(f"‚úÖ Built {len(docs)} documents")

    # Step 3: Split into chunks
    print("\n3Ô∏è‚É£  CREATING ENHANCED CHUNKS")
    splitter = EnhancedSplitter()
    chunks = splitter.split_documents(docs)
    print(f"‚úÖ Created {len(chunks)} enhanced chunks")

    # Step 4: Save data
    print("\n4Ô∏è‚É£  SAVING DATA")
    save_enhanced_data(chunks)

    # Step 5: Build QA system
    print("\n5Ô∏è‚É£  BUILDING QA SYSTEM")
    qa_system = EnhancedQASystem(api_key)
    qa_system.build_index(chunks)
    qa_system.create_qa_chain()

    # Step 6: Test system
    print("\n6Ô∏è‚É£  TESTING SYSTEM")
    test_questions = [
        "Welche Behandlungen werden von Ihnen angeboten?",
        "Was kostet eine Morpheus8 Behandlung?",
        "Welche Behandlungen gibt es speziell f√ºr M√§nner?",
        "Wie funktioniert Ultherapy und wie lange dauert es?",
        "Welche Behandlungen helfen gegen Falten?"
    ]

    for question in test_questions:
        qa_system.query(question)
        input("\nPress Enter for next question...")

    print("\nüéâ SYSTEM READY!")
    return qa_system

def save_enhanced_data(chunks):
    """Save enhanced data to files"""

    # Save JSON
    json_data = []
    for chunk in chunks:
        json_data.append({
            "chunk_id": chunk.metadata.get("chunk_id", 0),
            "text": chunk.page_content,
            "metadata": chunk.metadata
        })

    with open("enhanced_kb_chunks.json", "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    # Save CSV
    with open("enhanced_kb_chunks.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "chunk_id", "text", "source", "type", "importance", "has_treatments"
        ])
        writer.writeheader()

        for chunk in chunks:
            writer.writerow({
                "chunk_id": chunk.metadata.get("chunk_id", 0),
                "text": chunk.page_content,
                "source": chunk.metadata.get("source", ""),
                "type": chunk.metadata.get("type", ""),
                "importance": chunk.metadata.get("importance", 0),
                "has_treatments": chunk.metadata.get("has_treatments", False)
            })

    print("‚úÖ Data saved to enhanced_kb_chunks.json and enhanced_kb_chunks.csv")

# ==============================================
# 7. USAGE EXAMPLE
# ==============================================

if __name__ == "__main__":
    # Set your OpenAI API key here
    API_KEY = os.getenv('OPENAI_API_KEY')
    if not API_KEY:
        raise ValueError("OPENAI_API_KEY environment variable is required")

    # Run the complete pipeline
    qa_system = run_complete_pipeline(API_KEY, max_pages=50)  # Start with 50 pages for testing

    # Interactive mode
    print("\n" + "="*60)
    print("INTERACTIVE MODE - Ask your questions!")
    print("Type 'quit' to exit")
    print("="*60)

    while True:
        question = input("\nYour question: ")
        if question.lower() in ['quit', 'exit', 'q']:
            break

        try:
            qa_system.query(question)
        except Exception as e:
            print(f"Error: {e}")

    print("\nüëã Goodbye!")

# Create an improved chatbot implementation

import os
import json
import pandas as pd
from datetime import datetime
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.schema import Document

class ImprovedChatbot:
    def __init__(self, api_key, csv_file="enhanced_kb_chunks.csv"):
        """Initialize improved chatbot with memory and faster responses"""
        os.environ["OPENAI_API_KEY"] = api_key

        # Use faster, cheaper model
        self.llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",  # Much faster than gpt-4
            temperature=0.1,
            max_tokens=500  # Limit response length for speed
        )

        self.embeddings = OpenAIEmbeddings()

        # Add conversation memory (fixes memory issue)
        self.memory = ConversationBufferWindowMemory(
            k=5,  # Remember last 5 exchanges
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

        # Load and filter data
        self.load_and_filter_data(csv_file)
        self.build_vectorstore()
        self.create_chain()

    def load_and_filter_data(self, csv_file):
        """Load and filter chunks to improve quality"""
        df = pd.read_csv(csv_file)

        # Filter out low-quality chunks
        filtered_df = df[
            (df['importance'] >= 15) |  # High importance chunks
            (df['has_treatments'] == True) |  # Treatment-related content
            (df['type'].isin(['treatment_page', 'homepage', 'summary']))  # Important page types
        ].copy()

        # Remove navigation/menu noise
        filtered_df = filtered_df[
            ~filtered_df['text'].str.contains('Schlie√üen|Startseite|√úber uns', na=False, case=False)
        ]

        print(f"Filtered from {len(df)} to {len(filtered_df)} high-quality chunks")

        # Convert to documents
        self.documents = []
        for _, row in filtered_df.iterrows():
            doc = Document(
                page_content=row['text'],
                metadata={
                    'source': row['source'],
                    'type': row['type'],
                    'importance': row['importance']
                }
            )
            self.documents.append(doc)

    def build_vectorstore(self):
        """Build optimized vector store"""
        self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)
        print(f"Built vectorstore with {len(self.documents)} documents")

    def create_chain(self):
        """Create conversational chain with memory"""

        # Improved prompt template
        custom_prompt = PromptTemplate(
            template="""Du bist der AI-Assistent von Dr. med. Lara Pfahl f√ºr das Hautlabor Oldenburg.

Verwende den folgenden Kontext und die Gespr√§chshistorie, um die Frage zu beantworten:

Kontext: {context}

Gespr√§chshistorie: {chat_history}

Aktuelle Frage: {question}

Anweisungen:
- Gib pr√§zise, hilfreiche Antworten auf Deutsch
- Bei Behandlungsfragen: nenne Kosten, Dauer und Ablauf wenn verf√ºgbar
- Verweise auf Beratungstermine: +49 (0) 157 834 488 90
- Sei freundlich und professionell
- Halte Antworten fokussiert (max. 3-4 S√§tze)

Antwort:""",
            input_variables=["context", "chat_history", "question"]
        )

        # Optimized retriever (faster)
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",  # Faster than MMR
            search_kwargs={"k": 5}  # Fewer chunks = faster
        )

        # Create conversational chain
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": custom_prompt},
            return_source_documents=True,
            verbose=False
        )

        print("‚úÖ Conversational chain created with memory")

    def chat(self, question):
        """Chat with memory and context"""
        try:
            # Get response
            result = self.chain.invoke({"question": question})

            # Format response
            answer = result["answer"]
            sources = result.get("source_documents", [])

            print(f"\\nü§ñ Antwort: {answer}")

            if sources:
                print(f"\\nüìö Quellen:")
                for i, doc in enumerate(sources[:2], 1):  # Show top 2 sources
                    source = doc.metadata.get('source', 'Unbekannt')
                    print(f"  {i}. {source}")

            return answer

        except Exception as e:
            error_msg = f"Entschuldigung, es gab einen Fehler: {str(e)}"
            print(f"‚ùå {error_msg}")
            return error_msg

    def reset_memory(self):
        """Reset conversation memory"""
        self.memory.clear()
        print("üß† Gespr√§chshistorie zur√ºckgesetzt")

# Usage example
def run_improved_chatbot():
    """Run the improved chatbot"""

    # Initialize chatbot
    API_KEY = os.getenv('OPENAI_API_KEY')
    if not API_KEY:
        raise ValueError("OPENAI_API_KEY environment variable is required")

    try:
        chatbot = ImprovedChatbot(API_KEY)

        print("\\n" + "="*60)
        print("üöÄ VERBESSERTER HAUTLABOR CHATBOT")
        print("="*60)
        print("Verbesserungen:")
        print("‚úÖ Schnellere Antworten (GPT-3.5-turbo)")
        print("‚úÖ Gespr√§chsged√§chtnis (5 letzte Nachrichten)")
        print("‚úÖ Bessere Datenqualit√§t (gefilterte Chunks)")
        print("‚úÖ Fokussierte Antworten")
        print("\\nBefehle: 'reset' (Ged√§chtnis l√∂schen), 'quit' (beenden)")
        print("="*60)

        # Interactive chat loop
        while True:
            question = input("\\nüë§ Ihre Frage: ").strip()

            if question.lower() in ['quit', 'exit', 'q']:
                print("\\nüëã Auf Wiedersehen!")
                break
            elif question.lower() == 'reset':
                chatbot.reset_memory()
                continue
            elif not question:
                continue

            # Get response
            chatbot.chat(question)

    except Exception as e:
        print(f"‚ùå Fehler beim Starten: {e}")
        print("Bitte √ºberpr√ºfen Sie Ihren OpenAI API Key")

if __name__ == "__main__":
    run_improved_chatbot()


# Save the improved code to a file
with open('improved_chatbot.py', 'w', encoding='utf-8') as f:
    f.write(improved_code)

print("‚úÖ Improved chatbot code saved to 'improved_chatbot.py'")
print("\nKey improvements:")
print("1. üöÄ Speed: GPT-3.5-turbo instead of GPT-4")
print("2. üß† Memory: ConversationBufferWindowMemory")
print("3. üéØ Quality: Filtered chunks, better prompts")
print("4. ‚ö° Efficiency: Fewer retrieved chunks (5 vs 15)")